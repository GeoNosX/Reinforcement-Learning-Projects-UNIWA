{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZrXMdcFhk9p",
        "outputId": "4ccf8ede-26e3-4b84-8ea7-de66e0f98fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install tensorboardX\n",
        "!pip -q install ale-py\n",
        "!pip -q install \"gymnasium[atari,accept-rom-license,other]\" ale-py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DWkFe5nd1CH"
      },
      "source": [
        "# Some Wrappers\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DubxpLAmdzbJ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "\n",
        "# ========================\n",
        "# FireResetEnv\n",
        "# ========================\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        obs, info = self.env.reset(seed=seed, options=options)\n",
        "\n",
        "        # Gymnasium step returns 5 values\n",
        "        obs, _, terminated, truncated, info = self.env.step(1)\n",
        "        if terminated or truncated:\n",
        "            obs, info = self.env.reset()\n",
        "\n",
        "        obs, _, terminated, truncated, info = self.env.step(2)\n",
        "        if terminated or truncated:\n",
        "            obs, info = self.env.reset()\n",
        "\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "\n",
        "# ========================\n",
        "# MaxAndSkipEnv\n",
        "# ========================\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        info = {}\n",
        "\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        self._obs_buffer.clear()\n",
        "        obs, info = self.env.reset(seed=seed, options=options)\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs, info\n",
        "\n",
        "\n",
        "# ========================\n",
        "# ProcessFrame84\n",
        "# ========================\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = frame.reshape(210, 160, 3).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = frame.reshape(250, 160, 3).astype(np.float32)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown resolution.\")\n",
        "\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = x_t.reshape(84, 84, 1)\n",
        "\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "# ========================\n",
        "# ImageToPyTorch\n",
        "# ========================\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0,\n",
        "            high=1.0,\n",
        "            shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "# ========================\n",
        "# ScaledFloatFrame\n",
        "# ========================\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "# ========================\n",
        "# BufferWrapper\n",
        "# ========================\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=np.repeat(old_space.low, n_steps, axis=0),\n",
        "            high=np.repeat(old_space.high, n_steps, axis=0),\n",
        "            dtype=dtype,\n",
        "        )\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        obs, info = self.env.reset(seed=seed, options=options)\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(obs), info\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "# ========================\n",
        "# make_env\n",
        "# ========================\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name, render_mode=None)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    env = ScaledFloatFrame(env)\n",
        "    return env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciwDpaHtdlfQ"
      },
      "source": [
        "# The DQN\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jlabo4SKdc0s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_UKsUnMeHhk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGG28BmxeIZG"
      },
      "source": [
        "# The Agent And Training Loop\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyfRc5VqeQ9O",
        "outputId": "6f5ac598-7e22-4d84-9783-5c95ce911dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n",
            "137: done 1 games, mean reward 160.000, eps 1.00, speed 499.87 f/s\n",
            "229: done 2 games, mean reward 97.500, eps 1.00, speed 486.45 f/s\n",
            "436: done 3 games, mean reward 158.333, eps 1.00, speed 554.48 f/s\n",
            "510: done 4 games, mean reward 135.000, eps 0.99, speed 529.32 f/s\n",
            "729: done 5 games, mean reward 161.000, eps 0.99, speed 546.05 f/s\n",
            "Best mean reward updated 160.000 -> 161.000, model saved\n",
            "831: done 6 games, mean reward 137.500, eps 0.99, speed 509.90 f/s\n",
            "923: done 7 games, mean reward 125.000, eps 0.99, speed 533.27 f/s\n",
            "1037: done 8 games, mean reward 129.375, eps 0.99, speed 520.25 f/s\n",
            "1129: done 9 games, mean reward 120.556, eps 0.99, speed 512.48 f/s\n",
            "1253: done 10 games, mean reward 125.000, eps 0.99, speed 533.22 f/s\n",
            "1333: done 11 games, mean reward 116.364, eps 0.99, speed 522.41 f/s\n",
            "1446: done 12 games, mean reward 118.333, eps 0.99, speed 518.25 f/s\n",
            "1615: done 13 games, mean reward 127.692, eps 0.98, speed 548.00 f/s\n",
            "1744: done 14 games, mean reward 122.500, eps 0.98, speed 531.86 f/s\n",
            "1835: done 15 games, mean reward 117.333, eps 0.98, speed 519.91 f/s\n",
            "1961: done 16 games, mean reward 115.625, eps 0.98, speed 488.85 f/s\n",
            "2101: done 17 games, mean reward 116.765, eps 0.98, speed 522.07 f/s\n",
            "2258: done 18 games, mean reward 119.722, eps 0.98, speed 550.60 f/s\n",
            "2380: done 19 games, mean reward 118.947, eps 0.98, speed 275.39 f/s\n",
            "2445: done 20 games, mean reward 116.750, eps 0.98, speed 501.84 f/s\n",
            "2567: done 21 games, mean reward 119.762, eps 0.97, speed 517.93 f/s\n",
            "2700: done 22 games, mean reward 121.364, eps 0.97, speed 523.54 f/s\n",
            "2793: done 23 games, mean reward 118.913, eps 0.97, speed 517.94 f/s\n",
            "2907: done 24 games, mean reward 117.083, eps 0.97, speed 537.64 f/s\n",
            "3004: done 25 games, mean reward 115.000, eps 0.97, speed 537.17 f/s\n",
            "3112: done 26 games, mean reward 110.769, eps 0.97, speed 511.76 f/s\n",
            "3322: done 27 games, mean reward 116.667, eps 0.97, speed 521.37 f/s\n",
            "3550: done 28 games, mean reward 134.107, eps 0.96, speed 532.83 f/s\n",
            "3685: done 29 games, mean reward 133.793, eps 0.96, speed 508.38 f/s\n",
            "3864: done 30 games, mean reward 136.333, eps 0.96, speed 526.95 f/s\n",
            "4014: done 31 games, mean reward 136.935, eps 0.96, speed 530.09 f/s\n",
            "4193: done 32 games, mean reward 138.438, eps 0.96, speed 512.80 f/s\n",
            "4363: done 33 games, mean reward 140.606, eps 0.96, speed 511.36 f/s\n",
            "4514: done 34 games, mean reward 141.765, eps 0.95, speed 520.14 f/s\n",
            "4619: done 35 games, mean reward 139.714, eps 0.95, speed 446.75 f/s\n",
            "4740: done 36 games, mean reward 138.056, eps 0.95, speed 430.08 f/s\n",
            "4914: done 37 games, mean reward 141.892, eps 0.95, speed 463.33 f/s\n",
            "4987: done 38 games, mean reward 139.474, eps 0.95, speed 433.03 f/s\n",
            "5144: done 39 games, mean reward 146.923, eps 0.95, speed 476.78 f/s\n",
            "5353: done 40 games, mean reward 156.125, eps 0.95, speed 498.74 f/s\n",
            "5424: done 41 games, mean reward 153.537, eps 0.95, speed 470.12 f/s\n",
            "5555: done 42 games, mean reward 152.976, eps 0.94, speed 492.43 f/s\n",
            "5675: done 43 games, mean reward 151.395, eps 0.94, speed 498.84 f/s\n",
            "5856: done 44 games, mean reward 153.068, eps 0.94, speed 497.16 f/s\n",
            "6041: done 45 games, mean reward 154.333, eps 0.94, speed 496.84 f/s\n",
            "6133: done 46 games, mean reward 152.717, eps 0.94, speed 484.14 f/s\n",
            "6261: done 47 games, mean reward 151.170, eps 0.94, speed 482.73 f/s\n",
            "6374: done 48 games, mean reward 149.688, eps 0.94, speed 497.74 f/s\n",
            "6562: done 49 games, mean reward 153.163, eps 0.93, speed 527.80 f/s\n",
            "6657: done 50 games, mean reward 152.300, eps 0.93, speed 453.36 f/s\n",
            "6754: done 51 games, mean reward 150.686, eps 0.93, speed 477.72 f/s\n",
            "6964: done 52 games, mean reward 153.558, eps 0.93, speed 498.25 f/s\n",
            "7178: done 53 games, mean reward 157.075, eps 0.93, speed 509.83 f/s\n",
            "7381: done 54 games, mean reward 159.074, eps 0.93, speed 500.21 f/s\n",
            "7468: done 55 games, mean reward 157.000, eps 0.93, speed 455.41 f/s\n",
            "7532: done 56 games, mean reward 155.268, eps 0.92, speed 452.41 f/s\n",
            "7648: done 57 games, mean reward 153.246, eps 0.92, speed 460.61 f/s\n",
            "7766: done 58 games, mean reward 152.069, eps 0.92, speed 434.54 f/s\n",
            "7970: done 59 games, mean reward 154.153, eps 0.92, speed 498.47 f/s\n",
            "8109: done 60 games, mean reward 153.583, eps 0.92, speed 465.55 f/s\n",
            "8226: done 61 games, mean reward 151.803, eps 0.92, speed 466.08 f/s\n",
            "8337: done 62 games, mean reward 150.242, eps 0.92, speed 448.38 f/s\n",
            "8403: done 63 games, mean reward 148.730, eps 0.92, speed 403.64 f/s\n",
            "8459: done 64 games, mean reward 146.797, eps 0.92, speed 418.74 f/s\n",
            "8609: done 65 games, mean reward 147.385, eps 0.91, speed 484.59 f/s\n",
            "8756: done 66 games, mean reward 146.742, eps 0.91, speed 456.12 f/s\n",
            "8902: done 67 games, mean reward 146.194, eps 0.91, speed 475.21 f/s\n",
            "9032: done 68 games, mean reward 146.176, eps 0.91, speed 463.01 f/s\n",
            "9146: done 69 games, mean reward 146.304, eps 0.91, speed 469.07 f/s\n",
            "9204: done 70 games, mean reward 144.500, eps 0.91, speed 456.48 f/s\n",
            "9375: done 71 games, mean reward 145.845, eps 0.91, speed 391.83 f/s\n",
            "9476: done 72 games, mean reward 144.097, eps 0.91, speed 396.55 f/s\n",
            "9749: done 73 games, mean reward 145.000, eps 0.90, speed 472.50 f/s\n",
            "9871: done 74 games, mean reward 143.986, eps 0.90, speed 444.34 f/s\n",
            "10019: done 75 games, mean reward 142.800, eps 0.90, speed 230.57 f/s\n",
            "10144: done 76 games, mean reward 142.500, eps 0.90, speed 86.94 f/s\n",
            "10238: done 77 games, mean reward 141.299, eps 0.90, speed 87.05 f/s\n",
            "10365: done 78 games, mean reward 140.833, eps 0.90, speed 85.31 f/s\n",
            "10533: done 79 games, mean reward 141.709, eps 0.89, speed 85.06 f/s\n",
            "10676: done 80 games, mean reward 141.250, eps 0.89, speed 86.81 f/s\n",
            "10806: done 81 games, mean reward 141.358, eps 0.89, speed 84.39 f/s\n",
            "10983: done 82 games, mean reward 144.573, eps 0.89, speed 85.86 f/s\n",
            "11109: done 83 games, mean reward 144.277, eps 0.89, speed 84.91 f/s\n",
            "11201: done 84 games, mean reward 143.274, eps 0.89, speed 82.66 f/s\n",
            "11382: done 85 games, mean reward 143.529, eps 0.89, speed 81.71 f/s\n",
            "11543: done 86 games, mean reward 145.465, eps 0.88, speed 83.29 f/s\n",
            "11749: done 87 games, mean reward 149.023, eps 0.88, speed 80.42 f/s\n",
            "11849: done 88 games, mean reward 148.580, eps 0.88, speed 78.83 f/s\n",
            "11921: done 89 games, mean reward 147.360, eps 0.88, speed 74.23 f/s\n",
            "12045: done 90 games, mean reward 146.833, eps 0.88, speed 78.51 f/s\n",
            "12193: done 91 games, mean reward 146.593, eps 0.88, speed 82.30 f/s\n",
            "12316: done 92 games, mean reward 146.304, eps 0.88, speed 80.84 f/s\n",
            "12563: done 93 games, mean reward 148.333, eps 0.87, speed 82.11 f/s\n",
            "12699: done 94 games, mean reward 148.830, eps 0.87, speed 81.61 f/s\n",
            "12809: done 95 games, mean reward 148.263, eps 0.87, speed 81.37 f/s\n",
            "12987: done 96 games, mean reward 148.594, eps 0.87, speed 81.85 f/s\n",
            "13218: done 97 games, mean reward 149.691, eps 0.87, speed 80.25 f/s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 189\u001b[0m\n\u001b[0;32m    187\u001b[0m     loss_t \u001b[38;5;241m=\u001b[39m calc_loss(batch, net, tgt_net, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    188\u001b[0m     loss_t\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 189\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[1;32mc:\\Users\\geono\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\geono\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[1;32mc:\\Users\\geono\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Users\\geono\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\geono\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\geono\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\optim\\adam.py:611\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    609\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    614\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "DEFAULT_ENV_NAME = \"ALE/SpaceInvaders-v5\"\n",
        "MEAN_REWARD_BOUND = 500\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "\n",
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        # env.reset() in gymnasium returns (observation, info), so we unpack them.\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.asarray([self.state])\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        # env.step() in gymnasium returns (observation, reward, terminated, truncated, info)\n",
        "        new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "        is_done = terminated or truncated # Combine terminated and truncated for 'done'\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward\n",
        "\n",
        "\n",
        "def calc_loss(batch, net, tgt_net, device):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    states_v = torch.tensor(states).to(device)\n",
        "    next_states_v = torch.tensor(next_states).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "\n",
        "    # ✔ Create boolean mask FROM dones array\n",
        "    done_mask = torch.as_tensor(dones, dtype=torch.bool, device=device)\n",
        "\n",
        "    # Q(s, a)\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    # max_a' Q'(s', a')\n",
        "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "\n",
        "    # ✔ Zero out next_state_values where done=True\n",
        "    next_state_values = next_state_values.masked_fill(done_mask, 0.0)\n",
        "    next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = rewards_v + GAMMA * next_state_values\n",
        "\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
        "    parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "                        help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
        "    parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND,\n",
        "                        help=\"Mean reward boundary for stop of training, default=%.2f\" % MEAN_REWARD_BOUND)\n",
        "    args = parser.parse_args(['--cuda']) \n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "    env = make_env(args.env) # Changed to use the global make_env\n",
        "\n",
        "    net = DQN(env.observation_space.shape, env.action_space.n).to(device) # Changed to use global DQN\n",
        "    tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device) # Changed to use global DQN\n",
        "    writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "    print(net)\n",
        "\n",
        "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "    agent = Agent(env, buffer)\n",
        "    epsilon = EPSILON_START\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "    total_rewards = []\n",
        "    frame_idx = 0\n",
        "    ts_frame = 0\n",
        "    ts = time.time()\n",
        "    best_mean_reward = None\n",
        "\n",
        "    mean_rewards_filepath = args.env.replace(\"/\", \"_\") + \"_mean_rewards.csv\"\n",
        "\n",
        "\n",
        "    while True:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "        reward = agent.play_step(net, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            total_rewards.append(reward)\n",
        "            num_games = len(total_rewards)\n",
        "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "            ts_frame = frame_idx\n",
        "            ts = time.time()\n",
        "            mean_reward = np.mean(total_rewards[-100:])\n",
        "            print(f\"{frame_idx}: done {num_games} games, mean reward {mean_reward:.3f}, eps {epsilon:.2f}, speed {speed:.2f} f/s\")\n",
        "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
        "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "            # Save the mean reward to a file\n",
        "            if num_games % 100 == 0: # Save every 100 games\n",
        "                with open(mean_rewards_filepath, 'a') as f:\n",
        "                    f.write(f'{num_games},{mean_reward:.2f}\\n')\n",
        "                print(f'Mean reward for episode {num_games} saved to {mean_rewards_filepath}')\n",
        "\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "                torch.save(net.state_dict(), args.env.replace(\"/\", \"_\") + \"-best.dat\")\n",
        "\n",
        "                if best_mean_reward is not None:\n",
        "                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
        "                best_mean_reward = mean_reward\n",
        "            if mean_reward > args.reward:\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\n",
        "                break\n",
        "\n",
        "        if len(buffer) < REPLAY_START_SIZE:\n",
        "            continue\n",
        "\n",
        "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "            tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(BATCH_SIZE)\n",
        "        loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FARqxpfRglwR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7ChImTPkQdq"
      },
      "source": [
        "#Load Data And Retrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42EpXb0Vgkaa"
      },
      "outputs": [],
      "source": [
        "# Load the saved model weights\n",
        "model_path = args.env + \"-best.dat\"\n",
        "if os.path.exists(model_path):\n",
        "    net.load_state_dict(torch.load(model_path))\n",
        "    print(f\"Loaded model weights from {model_path}\")\n",
        "else:\n",
        "    print(f\"No saved model found at {model_path}, starting training from scratch.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4DWkFe5nd1CH",
        "ciwDpaHtdlfQ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
