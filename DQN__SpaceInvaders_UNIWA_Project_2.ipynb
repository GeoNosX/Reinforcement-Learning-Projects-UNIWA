{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "5JOD1mUZoKFg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gymnasium[atari,accept-rom-license]\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pChd49s7TONN",
        "outputId": "905415bc-c19b-48d7-867b-26bdaecc81ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.11.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ale-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPtthya-TC12",
        "outputId": "49e1c92d-2ed3-40a3-bf9a-5e8dbb3df8d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.12/dist-packages (from ale-py) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorboardX"
      ],
      "metadata": {
        "id": "LJjE2Tufo_bq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a206d5-5dc0-4120-9fa4-eb31c768f658"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3c7wUf6ETrko"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "from tensorboardX import SummaryWriter\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super().__init__(env)\n",
        "        self.n_steps = n_steps\n",
        "        self.dtype = dtype\n",
        "        self.frames = collections.deque(maxlen=n_steps)\n",
        "\n",
        "        # AtariPreprocessing without channel_first=True outputs (H, W)\n",
        "        obs_shape = env.observation_space.shape # (84, 84)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0,\n",
        "            high=1.0,\n",
        "            shape=(n_steps, obs_shape[0], obs_shape[1]), # Desired shape (4, 84, 84)\n",
        "            dtype=dtype\n",
        "        )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs) # obs is (84, 84)\n",
        "        # Initialize deque with the first observation repeated n_steps times\n",
        "        for _ in range(self.n_steps):\n",
        "            self.frames.append(obs)\n",
        "        return self._get_observation(), info # Return observation and info\n",
        "\n",
        "    def observation(self, observation): # observation is (84, 84)\n",
        "        self.frames.append(observation)\n",
        "        return self._get_observation()\n",
        "\n",
        "    def _get_observation(self):\n",
        "        # Stack frames along a new dimension (channel dimension) to get (n_steps, H, W)\n",
        "        return np.stack(list(self.frames), axis=0).astype(self.dtype)"
      ],
      "metadata": {
        "id": "RVLY5grLSemx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.wrappers import (\n",
        "    AtariPreprocessing,\n",
        "    TransformObservation,\n",
        "    RecordEpisodeStatistics\n",
        ")\n",
        "env = gym.make(\"ALE/SpaceInvaders-v5\", render_mode=\"rgb_array\", frameskip=1)\n",
        "# Apply AtariPreprocessing to convert to grayscale and resize frames\n",
        "env = AtariPreprocessing(env, screen_size=84, grayscale_newaxis=False, scale_obs=True)\n",
        "env = BufferWrapper(env, n_steps=4)"
      ],
      "metadata": {
        "id": "dlfkUN-DOoJq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(env.reset()[0][0], cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "OvNSR7VJgVfA",
        "outputId": "1239bbbd-d47d-466f-cd9d-99a5c8fc77e3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ac8a709a630>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "#First DQN\n",
        "___"
      ],
      "metadata": {
        "id": "5JOD1mUZoKFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__ (self, input_shape,n_actions):\n",
        "    super(DQN,self).__init__()\n",
        "\n",
        "    self.conv=nn.Sequential(\n",
        "        nn.Conv2d(input_shape[0],32,kernel_size=8,stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32,64,kernel_size=4,stride=2),\n",
        "        nn.ReLU()   )\n",
        "\n",
        "    conv_out_size=self._get_conv_out(input_shape)\n",
        "    self.fc=nn.Sequential(\n",
        "        nn.Linear(conv_out_size,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,n_actions)\n",
        "    )\n",
        "\n",
        "\n",
        "  def _get_conv_out(self,shape):\n",
        "    zeros=self.conv(torch.zeros(1,*shape))\n",
        "    return int(np.prod(zeros.size()))\n",
        "\n",
        "\n",
        "  def forward(self,frames):\n",
        "    conv_output=self.conv(frames)\n",
        "    conv_output=conv_output.view(conv_output.size()[0],-1)\n",
        "    return self.fc(conv_output)"
      ],
      "metadata": {
        "id": "aVvs6wGSYRRT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UqXh4ktkl_MV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "# ReplayBuffer\n",
        "\n",
        "___"
      ],
      "metadata": {
        "id": "-MFSU1DAoBOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_NAME=\"ALE/SpaceInvaders-v5\"\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "REPLAY_START_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "EPSILON_DECAY_LAST_FRAME = 150000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.01\n",
        "DEFAULT_ENV_NAME = \"ALE/SpaceInvaders-v5\"\n",
        "MEAN_REWARD_BOUND = 500"
      ],
      "metadata": {
        "id": "V74_b8o-oFnC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import replace\n",
        "Experience= collections.namedtuple(\"Experience\",field_names=['state','action','reward','done','new_state'])\n",
        "\n",
        "class ExperienceBuffer():\n",
        "\n",
        "  def __init__(self,capacity):\n",
        "    self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self,experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self,batch_size):\n",
        "    examples=np.random.choice(len(self.buffer), batch_size, replace=False) # Corrected arguments for np.random.choice\n",
        "    list_of_exp=[self.buffer[ex] for ex in examples]\n",
        "    states, actions, rewards, dones, next_states= zip(*list_of_exp)\n",
        "    return np.array(states), np.array(actions), np.array(rewards,dtype=np.float32), np.array(dones,dtype=np.uint8), np.array(next_states)"
      ],
      "metadata": {
        "id": "s_IqnLCG4BMG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Agent\n",
        "___"
      ],
      "metadata": {
        "id": "OFS-cgWxe2fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self,env,exp_buffer):\n",
        "    self.env=env\n",
        "    self.exp_buffer=exp_buffer\n",
        "    self._reset()\n",
        "\n",
        "  def _reset(self):\n",
        "    self.state, _ = self.env.reset() # Fix: env.reset() returns (observation, info) in Gymnasium\n",
        "    self.total_reward=0.0\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play_step(self,net,epsilon=0.0,device='cpu'):\n",
        "    done_reward=None\n",
        "\n",
        "    if np.random.random()<epsilon:\n",
        "      action=self.env.action_space.sample()\n",
        "    else:\n",
        "      state_a=np.asarray([self.state]) # Fix: Replaced np.array(..., copy=False) with np.asarray\n",
        "      state_v=torch.tensor(state_a).to(device)\n",
        "      q_vals_v=net(state_v)\n",
        "      _,act_v=torch.max(q_vals_v,dim=1)\n",
        "      action=int(act_v.item())\n",
        "    new_state,reward,terminated,truncated,info=self.env.step(action)\n",
        "    is_done = terminated or truncated\n",
        "    self.total_reward +=reward\n",
        "    exp=Experience(self.state,action,reward,is_done,new_state)\n",
        "    self.exp_buffer.append(exp)\n",
        "    self.state=new_state\n",
        "    if is_done:\n",
        "      done_reward=self.total_reward\n",
        "      self._reset() # Fix: Call _reset() method using self\n",
        "    return done_reward"
      ],
      "metadata": {
        "id": "OHIwalskBRKf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(batch,net,tgt_net,device='cpu'):\n",
        "  states,actions,rewards,dones,next_states=batch\n",
        "  states_v=torch.tensor(np.array(states,copy=False)).to(device)\n",
        "  next_statges_v=torch.tensor(np.array(next_states,copy=False)).to(device)\n",
        "  actions_v=torch.tensor(actions).to(device)\n",
        "  rewards_v=torch.tensor(rewards).to(device)\n",
        "  done_mask=torch.BoolTensor(dones).to(device)\n",
        "\n",
        "  state_action_values=net(states_v).gather(1,actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "  next_state_values=tgt_net(next_statges_v).max(1)[0]\n",
        "  next_state_values[done_mask]==0.0\n",
        "\n",
        "  next_state_values=next_state_values.detach()\n",
        "  expected_state_action_values=next_state_values*GAMMA+rewards_v\n",
        "  return nn.MSELoss()(state_action_values,expected_state_action_values)\n"
      ],
      "metadata": {
        "id": "XMxBOVRHxWth"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "# The Training Part\n",
        "___"
      ],
      "metadata": {
        "id": "Y76E85OZBHou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--cuda\", default=False,\n",
        "                        action=\"store_true\", help=\"Enable cuda\")\n",
        "    parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "                        help=\"Name of the environment, default=\" +\n",
        "                             DEFAULT_ENV_NAME)\n",
        "    args = parser.parse_args(args=[])\n",
        "    device=torch.device('cuda'if args.cuda else 'cpu')\n",
        "    net=DQN(env.observation_space.shape,env.action_space.n).to(device)\n",
        "    tgt_net=DQN(env.observation_space.shape,env.action_space.n).to(device)\n",
        "    writer=SummaryWriter(comment='-'+ args.env)\n",
        "    print(net)\n",
        "\n",
        "    buffer=ExperienceBuffer(REPLAY_SIZE)\n",
        "    agent=Agent(env,buffer)\n",
        "    optimizer=torch.optim.Adam(net.parameters(),lr=LEARNING_RATE)\n",
        "\n",
        "    total_rewards=[]\n",
        "    frame_idx=0\n",
        "    epsilon=EPSILON_START\n",
        "    best_m_reward=None\n",
        "\n",
        "    model_path = args.env.replace('/', '_') + \"-best.dat\"\n",
        "    # Load saved model weights and training progress if they exist\n",
        "    if os.path.exists(model_path):\n",
        "        try:\n",
        "            checkpoint = torch.load(model_path)\n",
        "            # Check if it's the new checkpoint format (a dictionary with 'net_state_dict')\n",
        "            if isinstance(checkpoint, dict) and 'net_state_dict' in checkpoint:\n",
        "                net.load_state_dict(checkpoint['net_state_dict'])\n",
        "                tgt_net.load_state_dict(checkpoint['net_state_dict'])\n",
        "                frame_idx = checkpoint['frame_idx']\n",
        "                best_m_reward = checkpoint['best_m_reward']\n",
        "                epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "                print(f\"Loaded model weights and progress from {model_path} (frame_idx: {frame_idx}, best_m_reward: {best_m_reward:.3f})\")\n",
        "            else: # Assume it's the old format where only state_dict was saved directly\n",
        "                net.load_state_dict(checkpoint)\n",
        "                tgt_net.load_state_dict(checkpoint)\n",
        "                print(f\"Loaded old format model weights from {model_path}. Training progress (frame_idx, best_m_reward, epsilon) will start from default.\")\n",
        "                # Reset frame_idx and best_m_reward as they are not available in old format\n",
        "                frame_idx = 0\n",
        "                best_m_reward = None\n",
        "                epsilon = EPSILON_START\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model from {model_path}: {e}. Starting training from scratch.\")\n",
        "            # Ensure these are reset if loading fails or for old format\n",
        "            frame_idx = 0\n",
        "            best_m_reward = None\n",
        "            epsilon = EPSILON_START\n",
        "\n",
        "    # Load episode rewards history if the file exists\n",
        "    episode_rewards_file_path = 'episode_rewards.txt'\n",
        "    if os.path.exists(episode_rewards_file_path):\n",
        "        with open(episode_rewards_file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    total_rewards.append(float(line.strip()))\n",
        "                except ValueError:\n",
        "                    continue # Skip invalid lines\n",
        "        print(f\"Loaded {len(total_rewards)} historical rewards from {episode_rewards_file_path}\")\n",
        "\n",
        "    ts_frame=0\n",
        "    ts=time.time()\n",
        "\n",
        "    # Open the file for appending new individual episode rewards\n",
        "    episode_rewards_file = open(episode_rewards_file_path, 'a')\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            frame_idx+=1\n",
        "            epsilon=max(EPSILON_FINAL,EPSILON_START-frame_idx/EPSILON_DECAY_LAST_FRAME)\n",
        "            reward=agent.play_step(net,epsilon,device=device)\n",
        "            if reward is not None:\n",
        "                total_rewards.append(reward)\n",
        "                # Save the individual episode reward to file\n",
        "                episode_rewards_file.write(f\"{reward}\\n\")\n",
        "                episode_rewards_file.flush() # Ensure it's written to disk immediately\n",
        "\n",
        "                speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "                ts_frame = frame_idx\n",
        "                ts = time.time()\n",
        "                m_reward = np.mean(total_rewards[-100:])\n",
        "                print(\"%d: done %d games, reward %.3f, \"\"eps %.2f, speed %.2f f/s\" % (frame_idx, len(total_rewards), m_reward, epsilon,speed))\n",
        "                writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "                writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "                writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "                writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "                if best_m_reward is None or best_m_reward < m_reward:\n",
        "                    checkpoint = {\n",
        "                        'net_state_dict': net.state_dict(),\n",
        "                        'frame_idx': frame_idx,\n",
        "                        'best_m_reward': m_reward,\n",
        "                    }\n",
        "                    torch.save(checkpoint, model_path)\n",
        "                    if best_m_reward is not None:\n",
        "                        print(\"Best reward updated %.3f -> %.3f\" % (best_m_reward, m_reward))\n",
        "                    best_m_reward = m_reward\n",
        "                if m_reward > MEAN_REWARD_BOUND:\n",
        "                    print(\"Solved in %d frames!\" % frame_idx)\n",
        "                    break\n",
        "            if len(buffer) < REPLAY_START_SIZE:\n",
        "                continue\n",
        "\n",
        "            if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "                tgt_net.load_state_dict(net.state_dict())\n",
        "            optimizer.zero_grad()\n",
        "            batch=buffer.sample(BATCH_SIZE)\n",
        "            loss_t=calc_loss(batch,net,tgt_net,device=device)\n",
        "            loss_t.backward()\n",
        "            optimizer.step()\n",
        "    finally:\n",
        "        # Ensure the file is closed even if an error occurs or training stops\n",
        "        episode_rewards_file.close()"
      ],
      "metadata": {
        "id": "swEeODE0BK8F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04a6548e-7465-4806-fafb-00d3a42cfb3c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=5184, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n",
            "Loaded old format model weights from ALE_SpaceInvaders-v5-best.dat. Training progress (frame_idx, best_m_reward, epsilon) will start from default.\n",
            "Loaded 209 historical rewards from episode_rewards.txt\n",
            "306: done 210 games, reward 175.100, eps 1.00, speed 845.48 f/s\n",
            "885: done 211 games, reward 176.100, eps 0.99, speed 872.98 f/s\n",
            "Best reward updated 175.100 -> 176.100\n",
            "1554: done 212 games, reward 174.900, eps 0.99, speed 865.00 f/s\n",
            "1993: done 213 games, reward 173.850, eps 0.99, speed 880.56 f/s\n",
            "2715: done 214 games, reward 175.350, eps 0.98, speed 901.88 f/s\n",
            "3413: done 215 games, reward 175.050, eps 0.98, speed 883.70 f/s\n",
            "4008: done 216 games, reward 175.650, eps 0.97, speed 854.80 f/s\n",
            "4399: done 217 games, reward 174.600, eps 0.97, speed 890.36 f/s\n",
            "4973: done 218 games, reward 175.700, eps 0.97, speed 578.22 f/s\n",
            "5502: done 219 games, reward 175.700, eps 0.96, speed 547.87 f/s\n",
            "6220: done 220 games, reward 175.100, eps 0.96, speed 723.25 f/s\n",
            "6628: done 221 games, reward 175.600, eps 0.96, speed 804.74 f/s\n",
            "7102: done 222 games, reward 175.350, eps 0.95, speed 829.65 f/s\n",
            "7817: done 223 games, reward 176.150, eps 0.95, speed 852.02 f/s\n",
            "Best reward updated 176.100 -> 176.150\n",
            "8189: done 224 games, reward 175.550, eps 0.95, speed 782.40 f/s\n",
            "8823: done 225 games, reward 175.500, eps 0.94, speed 819.16 f/s\n",
            "9495: done 226 games, reward 175.800, eps 0.94, speed 827.51 f/s\n",
            "10255: done 227 games, reward 179.450, eps 0.93, speed 46.34 f/s\n",
            "Best reward updated 176.150 -> 179.450\n",
            "10882: done 228 games, reward 180.250, eps 0.93, speed 13.66 f/s\n",
            "Best reward updated 179.450 -> 180.250\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-871758092.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mloss_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Ensure the file is closed even if an error occurs or training stops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m                             )\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    245\u001b[0m             )\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}